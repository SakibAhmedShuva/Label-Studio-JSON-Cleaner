{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file: c:\\Users\\Sakib Ahmed\\Downloads\\final_project-33-at-2024-11-26-23-07-87c5857c.json\n",
      "\n",
      "=== Special Characters Analysis Report ===\n",
      "\n",
      "Character  Count    Unicode Name                   Category   Hex Value \n",
      "----------------------------------------------------------------------\n",
      "           82       NO-BREAK SPACE                 Zs         0xa0      \n",
      "©          58       COPYRIGHT SIGN                 So         0xa9      \n",
      "‰          18       PER MILLE SIGN                 Po         0x2030    \n",
      "¬          8        NOT SIGN                       Sm         0xac      \n",
      "¢          7        CENT SIGN                      Sc         0xa2      \n",
      "š          7        LATIN SMALL LETTER S WITH CARO Ll         0x161     \n",
      "§          5        SECTION SIGN                   Po         0xa7      \n",
      "¨          4        DIAERESIS                      Sk         0xa8      \n",
      "‡          4        DOUBLE DAGGER                  Po         0x2021    \n",
      "‚          3        SINGLE LOW-9 QUOTATION MARK    Ps         0x201a    \n",
      "ƒ          2        LATIN SMALL LETTER F WITH HOOK Ll         0x192     \n",
      "´          1        ACUTE ACCENT                   Sk         0xb4      \n",
      "ª          1        FEMININE ORDINAL INDICATOR     Lo         0xaa      \n",
      "¡          1        INVERTED EXCLAMATION MARK      Po         0xa1      \n",
      "ˆ          1        MODIFIER LETTER CIRCUMFLEX ACC Lm         0x2c6     \n",
      "â          1        LATIN SMALL LETTER A WITH CIRC Ll         0xe2      \n",
      "\n",
      "Detailed report saved to 'special_chars_report.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "def analyze_special_characters(text):\n",
    "    \"\"\"\n",
    "    Analyze text for special characters and potential encoding issues\n",
    "    \"\"\"\n",
    "    # Find all non-ASCII characters\n",
    "    special_chars = re.findall(r'[^\\x00-\\x7F]+', text)\n",
    "    \n",
    "    # Create counter for special characters\n",
    "    char_counter = Counter(''.join(special_chars))\n",
    "    \n",
    "    # Analyze each character\n",
    "    char_analysis = []\n",
    "    for char, count in char_counter.items():\n",
    "        try:\n",
    "            name = unicodedata.name(char)\n",
    "            category = unicodedata.category(char)\n",
    "            hex_val = hex(ord(char))\n",
    "            char_analysis.append({\n",
    "                'character': char,\n",
    "                'count': count,\n",
    "                'unicode_name': name,\n",
    "                'category': category,\n",
    "                'hex_value': hex_val\n",
    "            })\n",
    "        except ValueError:\n",
    "            # Handle characters that can't be identified\n",
    "            char_analysis.append({\n",
    "                'character': char,\n",
    "                'count': count,\n",
    "                'unicode_name': 'UNKNOWN',\n",
    "                'category': 'UNKNOWN',\n",
    "                'hex_value': hex(ord(char))\n",
    "            })\n",
    "    \n",
    "    return char_analysis\n",
    "\n",
    "def find_special_chars_in_json(json_file):\n",
    "    \"\"\"\n",
    "    Find all special characters in a Label Studio JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Store all text content\n",
    "        all_text = []\n",
    "        \n",
    "        def extract_text(obj):\n",
    "            \"\"\"Recursively extract all text from JSON structure\"\"\"\n",
    "            if isinstance(obj, dict):\n",
    "                for value in obj.values():\n",
    "                    extract_text(value)\n",
    "            elif isinstance(obj, list):\n",
    "                for item in obj:\n",
    "                    extract_text(item)\n",
    "            elif isinstance(obj, str):\n",
    "                all_text.append(obj)\n",
    "        \n",
    "        # Extract all text from the JSON\n",
    "        extract_text(data)\n",
    "        \n",
    "        # Combine all text and analyze\n",
    "        combined_text = ' '.join(all_text)\n",
    "        analysis_results = analyze_special_characters(combined_text)\n",
    "        \n",
    "        # Sort by frequency\n",
    "        analysis_results.sort(key=lambda x: x['count'], reverse=True)\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing file: {str(e)}\"\n",
    "\n",
    "def print_analysis_report(analysis_results):\n",
    "    \"\"\"\n",
    "    Print a formatted report of special characters found\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Special Characters Analysis Report ===\\n\")\n",
    "    print(f\"{'Character':<10} {'Count':<8} {'Unicode Name':<30} {'Category':<10} {'Hex Value':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for result in analysis_results:\n",
    "        print(f\"{result['character']:<10} {result['count']:<8} {result['unicode_name'][:30]:<30} \"\n",
    "              f\"{result['category']:<10} {result['hex_value']:<10}\")\n",
    "\n",
    "def main():\n",
    "    # Replace with your JSON file path\n",
    "    json_file = r\"c:\\Users\\Sakib Ahmed\\Downloads\\final_project-33-at-2024-11-26-23-07-87c5857c.json\"\n",
    "    \n",
    "    print(f\"Analyzing file: {json_file}\")\n",
    "    results = find_special_chars_in_json(json_file)\n",
    "    \n",
    "    if isinstance(results, str):\n",
    "        print(f\"Error: {results}\")\n",
    "    else:\n",
    "        print_analysis_report(results)\n",
    "        \n",
    "        # Save results to a file\n",
    "        with open('special_chars_report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(\"\\nDetailed report saved to 'special_chars_report.json'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved to 10.json\n"
     ]
    }
   ],
   "source": [
    "def fix_encoding(text):\n",
    "    \"\"\"\n",
    "    Carefully fix common encoding issues without changing text structure.\n",
    "    Handles various special characters, symbols, and encoding artifacts.\n",
    "    \"\"\"\n",
    "    # Mapping of common problematic character sequences\n",
    "    encoding_fixes = {\n",
    "        # Original mappings\n",
    "        'ÃƒÂ': 'A',\n",
    "        'Ã': 'A',\n",
    "        'ƒÂ': '',\n",
    "        'â€™': \"'\",  # Smart quote\n",
    "        'â€œ': '\"',  # Left double quote\n",
    "        'â€': '\"',   # Right double quote\n",
    "        'Ã©': 'e',\n",
    "        'Ã¨': 'e',\n",
    "        'Â': ' ',\n",
    "\n",
    "        # Space and formatting characters\n",
    "        '\\xa0': ' ',    # NO-BREAK SPACE\n",
    "        '\\u2028': ' ',  # LINE SEPARATOR\n",
    "        '\\u2029': ' ',  # PARAGRAPH SEPARATOR\n",
    "\n",
    "        # Common symbols\n",
    "        '©': '(c)',     # COPYRIGHT SIGN\n",
    "        '‰': '%',       # PER MILLE SIGN\n",
    "        '¬': '-',       # NOT SIGN\n",
    "        '¢': 'c',       # CENT SIGN\n",
    "        'š': 's',       # LATIN SMALL LETTER S WITH CARON\n",
    "        '§': 'S',       # SECTION SIGN\n",
    "        '¨': '',        # DIAERESIS\n",
    "        '‡': '+',       # DOUBLE DAGGER\n",
    "        '‚': \"'\",       # SINGLE LOW-9 QUOTATION MARK\n",
    "        'ƒ': 'f',       # LATIN SMALL LETTER F WITH HOOK\n",
    "        '´': \"'\",       # ACUTE ACCENT\n",
    "        'ª': 'a',       # FEMININE ORDINAL INDICATOR\n",
    "        '¡': '!',       # INVERTED EXCLAMATION MARK\n",
    "        'ˆ': '^',       # MODIFIER LETTER CIRCUMFLEX ACCENT\n",
    "        'â': 'a',       # LATIN SMALL LETTER A WITH CIRCUMFLEX\n",
    "\n",
    "        # Additional clean-up for multiple spaces and line breaks\n",
    "        '  ': ' ',      # Double space to single space\n",
    "        '\\r\\n': '\\n',   # Normalize line endings\n",
    "        '\\r': '\\n',     # Carriage return to newline\n",
    "    }\n",
    "    \n",
    "    # Apply replacements\n",
    "    for bad_char, replacement in encoding_fixes.items():\n",
    "        text = text.replace(bad_char, replacement)\n",
    "    \n",
    "    # Final cleanup for any remaining double spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_json_with_annotations(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Clean encoding while preserving JSON structure and annotations\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Read the original file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Recursive cleaning function\n",
    "    def deep_clean(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: deep_clean(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [deep_clean(item) for item in obj]\n",
    "        elif isinstance(obj, str):\n",
    "            return fix_encoding(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Clean the entire data structure\n",
    "    cleaned_data = deep_clean(data)\n",
    "    \n",
    "    # Write the cleaned data\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"File cleaned and saved to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "input_file = r'd:\\OneDrive - Personal\\final.json'\n",
    "output_file = 'cleaned_pastde.json'\n",
    "clean_json_with_annotations(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File cleaned and saved to 1.json\n"
     ]
    }
   ],
   "source": [
    "def fix_encoding(text, annotations=None):\n",
    "    \"\"\"\n",
    "    Fix encoding issues while preserving annotation positions.\n",
    "    Returns cleaned text and adjusted annotations.\n",
    "    \"\"\"\n",
    "    # Create a mapping of original positions to new positions\n",
    "    position_map = {}\n",
    "    current_pos = 0\n",
    "    cleaned_text = \"\"\n",
    "    \n",
    "    # Process text character by character\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        matched = False\n",
    "        \n",
    "        # Check for multi-character replacements first\n",
    "        for bad_char, replacement in encoding_fixes.items():\n",
    "            if text[i:i+len(bad_char)] == bad_char:\n",
    "                position_map[i] = current_pos\n",
    "                cleaned_text += replacement\n",
    "                current_pos += len(replacement)\n",
    "                i += len(bad_char)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If no multi-character replacement found, process single character\n",
    "        if not matched:\n",
    "            position_map[i] = current_pos\n",
    "            if text[i] in ['\\xa0', '\\u2028', '\\u2029']:\n",
    "                cleaned_text += ' '\n",
    "            else:\n",
    "                cleaned_text += text[i]\n",
    "            current_pos += 1\n",
    "            i += 1\n",
    "    \n",
    "    # Adjust annotations if provided\n",
    "    if annotations:\n",
    "        adjusted_annotations = []\n",
    "        for ann in annotations:\n",
    "            start = position_map.get(ann['start'], ann['start'])\n",
    "            end = position_map.get(ann['end'], ann['end'])\n",
    "            adjusted_ann = ann.copy()\n",
    "            adjusted_ann.update({'start': start, 'end': end})\n",
    "            adjusted_annotations.append(adjusted_ann)\n",
    "        return cleaned_text, adjusted_annotations\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def clean_json_with_annotations(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Clean encoding while preserving JSON structure and annotations\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Read the original file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    def deep_clean(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            # Check if this is an annotated text entry\n",
    "            if 'text' in obj and 'annotations' in obj:\n",
    "                cleaned_text, adjusted_annotations = fix_encoding(\n",
    "                    obj['text'], \n",
    "                    obj['annotations']\n",
    "                )\n",
    "                return {\n",
    "                    **obj,\n",
    "                    'text': cleaned_text,\n",
    "                    'annotations': adjusted_annotations\n",
    "                }\n",
    "            return {k: deep_clean(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [deep_clean(item) for item in obj]\n",
    "        elif isinstance(obj, str):\n",
    "            return fix_encoding(obj)\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    # Clean the entire data structure\n",
    "    cleaned_data = deep_clean(data)\n",
    "    \n",
    "    # Write the cleaned data\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"File cleaned and saved to {output_file}\")\n",
    "\n",
    "# The encoding_fixes dictionary remains the same as in your original code\n",
    "encoding_fixes = {\n",
    "    # Original mappings\n",
    "    'ÃƒÂ': 'A',\n",
    "    'Ã': 'A',\n",
    "    'ƒÂ': '',\n",
    "    'â€™': \"'\",  # Smart quote\n",
    "    'â€œ': '\"',  # Left double quote\n",
    "    'â€': '\"',   # Right double quote\n",
    "    'Ã©': 'e',\n",
    "    'Ã¨': 'e',\n",
    "    'Â': ' ',\n",
    "\n",
    "    # Space and formatting characters\n",
    "    '\\xa0': ' ',    # NO-BREAK SPACE\n",
    "    '\\u2028': ' ',  # LINE SEPARATOR\n",
    "    '\\u2029': ' ',  # PARAGRAPH SEPARATOR\n",
    "\n",
    "    # Common symbols\n",
    "    '©': '(c)',     # COPYRIGHT SIGN\n",
    "    '‰': '%',       # PER MILLE SIGN\n",
    "    '¬': '-',       # NOT SIGN\n",
    "    '¢': 'c',       # CENT SIGN\n",
    "    'š': 's',       # LATIN SMALL LETTER S WITH CARON\n",
    "    '§': 'S',       # SECTION SIGN\n",
    "    '¨': '',        # DIAERESIS\n",
    "    '‡': '+',       # DOUBLE DAGGER\n",
    "    '‚': \"'\",       # SINGLE LOW-9 QUOTATION MARK\n",
    "    'ƒ': 'f',       # LATIN SMALL LETTER F WITH HOOK\n",
    "    '´': \"'\",       # ACUTE ACCENT\n",
    "    'ª': 'a',       # FEMININE ORDINAL INDICATOR\n",
    "    '¡': '!',       # INVERTED EXCLAMATION MARK\n",
    "    'ˆ': '^',       # MODIFIER LETTER CIRCUMFLEX ACCENT\n",
    "    'â': 'a',       # LATIN SMALL LETTER A WITH CIRCUMFLEX\n",
    "\n",
    "    # Additional clean-up for multiple spaces and line breaks\n",
    "    '  ': ' ',      # Double space to single space\n",
    "    '\\r\\n': '\\n',   # Normalize line endings\n",
    "    '\\r': '\\n',     # Carriage return to newline\n",
    "}\n",
    "\n",
    "# Usage\n",
    "input_file = r'd:\\OneDrive - Personal\\FleetBlox\\Data\\Driving Licences\\Final Json\\train_USA.json'\n",
    "output_file = '1.json'\n",
    "clean_json_with_annotations(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
